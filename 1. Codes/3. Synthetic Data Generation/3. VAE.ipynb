{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DFCQSa98yMu7"
   },
   "source": [
    "# **Data Loading and Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HGu4n59NjOK2"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Replace 'your_dataset.csv' with the actual path to your CSV file\n",
    "file_path = '/content/imputed_encoded_dataset.csv'\n",
    "new_file_path = 'cleaned_dataset.csv'  # Define the path for the cleaned dataset\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Drop rows with missing values in these columns\n",
    "df_cleaned = df.dropna()\n",
    "\n",
    "# Save the cleaned dataset to a new CSV file, without the index\n",
    "df_cleaned.to_csv(new_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V7ENgraio7gN",
    "outputId": "cff8c968-7253-4f99-8af5-cc7d91e9996f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(75739, 68)\n"
     ]
    }
   ],
   "source": [
    "print(df_cleaned.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l_Jn3wHByZev"
   },
   "source": [
    "# **Synthetic Data Generation Using VAE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lgTaKjuYucdI"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv('/content/cleaned_dataset.csv')\n",
    "\n",
    "# Define the list of continuous columns\n",
    "continuous_cols = ['careplan_length','Diastolic Blood Pressure','Systolic Blood Pressure','Body Mass Index','Total Cholesterol','High Density Lipoprotein Cholesterol','Triglycerides','Low Density Lipoprotein Cholesterol','Glucose','Hemoglobin A1c/Hemoglobin.total in Blood','Sodium','Chloride','Potassium','Carbon Dioxide','Calcium','Urea Nitrogen','Estimated Glomerular Filtration Rate']  # Replace [...] with your continuous column names\n",
    "\n",
    "# Assuming all other columns are binary except the continuous ones\n",
    "all_cols = df.columns.tolist()\n",
    "binary_cols = [col for col in all_cols if col not in continuous_cols and col != 'Viral_sinusitis_present']  # Exclude target if it's part of the DataFrame\n",
    "\n",
    "# Preprocess the data\n",
    "continuous_data = df[continuous_cols].values\n",
    "binary_data = df[binary_cols].values\n",
    "\n",
    "# Standardizing continuous variables\n",
    "scaler = StandardScaler()\n",
    "continuous_data = scaler.fit_transform(continuous_data)\n",
    "\n",
    "# Splitting the dataset\n",
    "cont_train, cont_test, binary_train, binary_test = train_test_split(continuous_data, binary_data, test_size=0.2, random_state=42)\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, continuous_data, binary_data):\n",
    "        self.continuous_data = continuous_data\n",
    "        self.binary_data = binary_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.continuous_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.continuous_data[idx], self.binary_data[idx]\n",
    "\n",
    "# Creating datasets and dataloaders\n",
    "train_dataset = CustomDataset(torch.tensor(cont_train, dtype=torch.float), torch.tensor(binary_train, dtype=torch.float))\n",
    "test_dataset = CustomDataset(torch.tensor(cont_test, dtype=torch.float), torch.tensor(binary_test, dtype=torch.float))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, continuous_dims, binary_dims, hidden_dims=256, latent_dims=64):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(continuous_dims + binary_dims, hidden_dims),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dims, latent_dims * 2)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dims, hidden_dims),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dims, continuous_dims + binary_dims)\n",
    "        )\n",
    "\n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        mu, log_var = encoded.chunk(2, dim=-1)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        decoded = self.decoder(z)\n",
    "        return decoded, mu, log_var\n",
    "\n",
    "def loss_function(recon_x, x, mu, log_var, continuous_dims):\n",
    "    recon_x_cont, recon_x_binary = recon_x.split([continuous_dims, x.size(1)-continuous_dims], dim=1)\n",
    "    x_cont, x_binary = x.split([continuous_dims, x.size(1)-continuous_dims], dim=1)\n",
    "    BCE = nn.BCEWithLogitsLoss()(recon_x_binary, x_binary)\n",
    "    MSE = nn.MSELoss()(recon_x_cont, x_cont)\n",
    "    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "    return BCE + MSE + KLD\n",
    "\n",
    "model = VAE(len(continuous_cols), len(binary_cols))\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "epochs = 100  # Adjust based on your needs\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for cont_data, binary_data in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        recon_data, mu, log_var = model(torch.cat((cont_data, binary_data), dim=1))\n",
    "        loss = loss_function(recon_data, torch.cat((cont_data, binary_data), dim=1), mu, log_var, len(continuous_cols))\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch}, Loss: {train_loss / len(train_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OtHiC81m6XsS"
   },
   "outputs": [],
   "source": [
    "def generate_synthetic_data(model, n_samples, scaler, continuous_dims, binary_dims):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        # Sample from the latent space\n",
    "        z = torch.randn(n_samples, 64)  # Adjust the size as per your model's latent dimensions\n",
    "        # Decode the sampled latent variables\n",
    "        synthetic_data = model.decoder(z)\n",
    "\n",
    "        # Ensure the model's decoder output is handled correctly:\n",
    "        # The decoder should output a tensor that combines both continuous and binary data,\n",
    "        # which we then need to split.\n",
    "        synthetic_cont, synthetic_binary_logits = synthetic_data.split([continuous_dims, binary_dims], dim=1)\n",
    "\n",
    "        # Sigmoid activation to convert logits to probabilities for binary data\n",
    "        synthetic_binary = torch.sigmoid(synthetic_binary_logits)\n",
    "\n",
    "        # Convert probabilities to binary (0 or 1) based on a threshold (0.5)\n",
    "        synthetic_binary = (synthetic_binary > 0.5).float()\n",
    "\n",
    "        # Inverse transform the continuous variables to their original scale\n",
    "        synthetic_cont_np = synthetic_cont.cpu().numpy()\n",
    "        synthetic_cont_np = scaler.inverse_transform(synthetic_cont_np)\n",
    "\n",
    "        synthetic_binary_np = synthetic_binary.cpu().numpy()\n",
    "\n",
    "        return synthetic_cont_np, synthetic_binary_np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DrghADLC7n0n"
   },
   "outputs": [],
   "source": [
    "# Generate synthetic data\n",
    "n_synthetic_samples = 1000  # The desired number of synthetic samples\n",
    "synthetic_cont, synthetic_binary = generate_synthetic_data(model, n_synthetic_samples, scaler, continuous_dims, binary_dims)\n",
    "\n",
    "# Combine continuous and binary parts\n",
    "synthetic_data_combined = np.hstack((synthetic_cont, synthetic_binary))\n",
    "\n",
    "# Create a DataFrame with appropriate column names and save to CSV\n",
    "synthetic_df = pd.DataFrame(synthetic_data_combined, columns=continuous_cols + binary_cols)\n",
    "synthetic_df.to_csv('synthetic_dataset.csv', index=False)\n",
    "\n",
    "print(\"Synthetic dataset generated and saved to synthetic_dataset.csv.\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
